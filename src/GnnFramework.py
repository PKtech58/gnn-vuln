#!/usr/bin/env python3
"""
LLVM-IRè„†å¼±æ€§æ¤œå‡ºGNNãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - æœ€çµ‚ç‰ˆ
äº¤å·®æ¤œè¨¼ãƒ»å€‹åˆ¥CWEæ¤œçŸ¥ãƒ»ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’å¯¾å¿œ

å¼•æ•°ä»•æ§˜:
--data-path: JSONãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
--approach: æ¤œçŸ¥æ–¹å¼ (multi, CWE-416, CWE-119, CWE-476, CWE-190, CWE-78)
--mode: å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (validate=äº¤å·®æ¤œè¨¼, train=å­¦ç¿’, test=æ¤œçŸ¥)
--output-dir: çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
"""

import os
import re
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, Batch
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GCNConv, GATConv, GraphConv, BatchNorm
import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
import traceback
import logging
from datetime import datetime
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def normalize_path(path):
    """ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã‚’çµ±ä¸€ï¼ˆWindows/Linuxå¯¾å¿œï¼‰"""
    if not path:
        return path
    normalized = path.replace('\\\\', '/').replace('\\', '/')
    normalized = re.sub('/+', '/', normalized)
    return normalized


def extract_clean_filename(path):
    """æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º"""
    normalized = normalize_path(path)
    return os.path.basename(normalized)



class VulnerabilityDataset:
    """è„†å¼±æ€§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆäº¤å·®æ¤œè¨¼ãƒ»å€‹åˆ¥CWEå¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self, approach='multi'):
        self.approach = approach
        self.all_target_cwes = ['CWE-416', 'CWE-119', 'CWE-476', 'CWE-190', 'CWE-78']
        
        if approach == 'multi':
            self.target_cwes = self.all_target_cwes
            self.num_classes = 5
        elif approach in self.all_target_cwes:
            self.target_cwes = [approach]
            self.num_classes = 1
        else:
            raise ValueError(f"ç„¡åŠ¹ãªapproach: {approach}")
        
        self.graphs = []
        self.file_groups = defaultdict(list)
        
        logger.info(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–: approach={approach}, classes={self.num_classes}")
    
    def extract_filename_from_item(self, item):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºï¼ˆ7ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰"""
        patterns = [
            lambda x: x.get('file_name'),
            lambda x: x.get('file_path'),
            lambda x: x.get('path'),
            lambda x: x.get('source_file'),
            lambda x: x.get('llvm_file_path'),
            lambda x: x.get('metadata', {}).get('file_name') if isinstance(x.get('metadata'), dict) else None,
            lambda x: self._infer_filename_from_url_or_id(x)
        ]
        
        for i, pattern in enumerate(patterns):
            try:
                result = pattern(item)
                if result:
                    filename = extract_clean_filename(result)
                    logger.debug(f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã§ãƒ•ã‚¡ã‚¤ãƒ«åæŠ½å‡º: {filename}")
                    return filename
            except Exception as e:
                logger.debug(f"ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        item_hash = hash(str(item))
        fallback_name = f"item_{abs(item_hash)}.ll"
        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åæŠ½å‡ºå¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {fallback_name}")
        return fallback_name
    
    def _infer_filename_from_url_or_id(self, item):
        """URL ã¾ãŸã¯IDã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨å®š"""
        test_id = item.get('test_id_from_llvm', item.get('test_id'))
        if test_id:
            return f"test_{test_id}.ll"
        
        for key, value in item.items():
            if isinstance(value, str) and ('.ll' in value or '.c' in value):
                return extract_clean_filename(value)
        
        return None
    
    def load_data(self, json_path):
        """JSONãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict) and 'dataset' in data:
                dataset = data['dataset']
            elif isinstance(data, list):
                dataset = data
            else:
                raise ValueError("ç„¡åŠ¹ãªJSONãƒ‡ãƒ¼ã‚¿æ§‹é€ ")
            
            logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¦ç´ æ•°: {len(dataset)}")
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed_count = 0
            error_count = 0
            
            for i, item in enumerate(dataset):
                try:
                    if not isinstance(item, dict):
                        error_count += 1
                        continue
                    
                    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self._should_include_item(item):
                        continue
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åæŠ½å‡ºã¨ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                    filename = self.extract_filename_from_item(item)
                    self.file_groups[filename].append(self._process_item(item, i))
                    processed_count += 1
                    
                except Exception as e:
                    logger.warning(f"ã‚¢ã‚¤ãƒ†ãƒ {i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    error_count += 1
            
            logger.info(f"âœ… å‡¦ç†å®Œäº†: {processed_count}ä»¶æˆåŠŸ, {error_count}ä»¶ã‚¨ãƒ©ãƒ¼")
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(self.file_groups)}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°çµ±è¨ˆè¡¨ç¤º
            self._show_file_mapping_stats()
            
            # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            self._build_graphs()
            
            return len(self.graphs)
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _should_include_item(self, item):
        """ã‚¢ãƒ—ãƒ­ãƒ¼ãƒè¨­å®šã«åŸºã¥ãã‚¢ã‚¤ãƒ†ãƒ åŒ…å«åˆ¤å®š"""
        if self.approach == 'multi':
            return True
        
        vuln_info = item.get('vulnerability_info', {})
        if isinstance(vuln_info, dict):
            cwe_id = vuln_info.get('cwe_id') or vuln_info.get('target_cwe')
            return cwe_id == self.approach
        
        return False
    
    def _process_item(self, item, index):
        """ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ï¼ˆãƒ©ãƒ™ãƒ«æŠ½å‡ºå«ã‚€ï¼‰"""
        cwe_labels, trigger_labels = self._extract_labels(item)
        
        processed_item = item.copy()
        processed_item.update({
            'cwe_labels': cwe_labels,
            'trigger_labels': trigger_labels,
            'item_index': index,
        })
        
        return processed_item
    
    def _extract_labels(self, item):
        """CWE-IDãƒ©ãƒ™ãƒ«ã¨is_trigger_lineãƒ©ãƒ™ãƒ«ã®æŠ½å‡º"""
        state = item.get('vulnerability_state', item.get('STATE', 'good'))
        ir_line_index = item.get('ir_line_index', 0)
        
        # åˆæœŸåŒ–: 5ã¤ã®CWE-IDã«å¯¾å¿œ
        cwe_labels = [0.0] * len(self.all_target_cwes)
        trigger_labels = [0.0] * len(self.all_target_cwes)
        
        if state == 'bad':
            vuln_info = item.get('vulnerability_info', {})
            
            if isinstance(vuln_info, dict):
                # CWE-IDå‡¦ç†
                cwe_id = vuln_info.get('cwe_id') or vuln_info.get('target_cwe')
                if cwe_id and cwe_id in self.all_target_cwes:
                    idx = self.all_target_cwes.index(cwe_id)
                    cwe_labels[idx] = 1.0
                
                # is_trigger_lineå‡¦ç†
                is_trigger_line = vuln_info.get('is_trigger_line', False)
                trigger_lines = vuln_info.get('trigger_lines', [])
                
                # ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³åˆ¤å®š
                if is_trigger_line or (trigger_lines and ir_line_index in trigger_lines):
                    if cwe_id and cwe_id in self.all_target_cwes:
                        idx = self.all_target_cwes.index(cwe_id)
                        trigger_labels[idx] = 1.0
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒè¨­å®šã«å¿œã˜ãŸèª¿æ•´
        if self.approach != 'multi':
            target_idx = self.all_target_cwes.index(self.approach)
            return [cwe_labels[target_idx]], [trigger_labels[target_idx]]
        
        return cwe_labels, trigger_labels
    
    def _show_file_mapping_stats(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°çµ±è¨ˆè¡¨ç¤º"""
        logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°çµ±è¨ˆ:")
        logger.info(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(self.file_groups)}")
        
        # ä¸Šä½5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
        sorted_files = sorted(self.file_groups.items(), key=lambda x: len(x[1]), reverse=True)
        for filename, items in sorted_files[:5]:
            logger.info(f"   {filename}: {len(items)} items")
        
        if len(sorted_files) > 5:
            logger.info(f"   ... ä»– {len(sorted_files) - 5} ãƒ•ã‚¡ã‚¤ãƒ«")
    
    def _build_graphs(self):
        """ã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        logger.info(f"ğŸ”§ ã‚°ãƒ©ãƒ•æ§‹ç¯‰é–‹å§‹: {len(self.file_groups)} files")
        
        built_count = 0
        for filename, items in self.file_groups.items():
            try:
                if len(items) < 1:
                    continue
                
                graph = self._build_single_graph(items, filename)
                if graph is not None:
                    self.graphs.append(graph)
                    built_count += 1
                    
            except Exception as e:
                logger.warning(f"ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼ ({filename}): {e}")
        
        logger.info(f"âœ… ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†: {built_count} graphs")
    
    def _build_single_graph(self, items, filename):
        """å˜ä¸€ã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        if len(items) == 0:
            return None
        
        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«
        node_features = []
        node_labels = []
        node_metadata = []
        
        for item in items:
            # ç‰¹å¾´é‡æ§‹ç¯‰ï¼ˆ512æ¬¡å…ƒï¼‰
            feature_vector = [
                item.get('ir_line_index', 0) % 1000,
                len(str(item.get('ir_content', ''))),
                hash(item.get('type', 'unknown')) % 1000,
                1.0 if item.get('is_vulnerable', False) else 0.0
            ]
            
            # 512æ¬¡å…ƒã«æ‹¡å¼µ
            while len(feature_vector) < 512:
                feature_vector.append(0.0)
            
            node_features.append(feature_vector)
            
            # ãƒ©ãƒ™ãƒ«ï¼ˆCWE + ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³ï¼‰
            cwe_labels = item.get('cwe_labels', [0] * self.num_classes)
            trigger_labels = item.get('trigger_labels', [0] * self.num_classes)
            combined_labels = cwe_labels + trigger_labels
            node_labels.append(combined_labels)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            node_metadata.append({
                'filename': filename,
                'ir_line': item.get('ir_line_index', 0),
                'item_index': item.get('item_index', 0),
            })
        
        # ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆé †æ¬¡æ¥ç¶šï¼‰
        edges = []
        edge_attrs = []
        
        for i in range(len(items) - 1):
            edges.append([i, i + 1])
            edge_attrs.append([1.0, 0.0])  # control flow
        
        # PyTorch Geometricãƒ‡ãƒ¼ã‚¿ä½œæˆ
        x = torch.tensor(node_features, dtype=torch.float)
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)
        edge_attr = torch.tensor(edge_attrs, dtype=torch.float) if edge_attrs else None
        y = torch.tensor(node_labels, dtype=torch.float)
        
        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, metadata=node_metadata)
    
    def get_cross_validation_splits(self, k_folds=5):
        """äº¤å·®æ¤œè¨¼ç”¨åˆ†å‰²"""
        if len(self.file_groups) < k_folds:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°({len(self.file_groups)})ãŒfoldæ•°({k_folds})ã‚ˆã‚Šå°‘ãªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«èª¿æ•´")
            k_folds = len(self.file_groups)
        
        files = list(self.file_groups.keys())
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        
        splits = []
        for fold, (train_files_idx, val_files_idx) in enumerate(kf.split(files)):
            train_files = [files[i] for i in train_files_idx]
            val_files = [files[i] for i in val_files_idx]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            train_indices = self._get_graph_indices_by_files(train_files)
            val_indices = self._get_graph_indices_by_files(val_files)
            
            splits.append({
                'fold': fold + 1,
                'train_indices': train_indices,
                'val_indices': val_indices,
                'train_files': train_files,
                'val_files': val_files
            })
            
            logger.info(f"Fold {fold + 1}: Train={len(train_indices)}, Val={len(val_indices)}")
        
        return splits
    
    def get_train_test_split(self, test_size=0.2):
        """é€šå¸¸ã®å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²"""
        files = list(self.file_groups.keys())
        
        if len(files) < 2:
            logger.warning("ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå°‘ãªã„ãŸã‚ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã«ä½¿ç”¨")
            return list(range(len(self.graphs))), []
        
        train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)
        
        train_indices = self._get_graph_indices_by_files(train_files)
        test_indices = self._get_graph_indices_by_files(test_files)
        
        logger.info(f"ğŸ“Š åˆ†å‰²çµæœ: Train={len(train_indices)}, Test={len(test_indices)}")
        
        return train_indices, test_indices
    
    def _get_graph_indices_by_files(self, target_files):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—"""
        indices = []
        
        for i, graph in enumerate(self.graphs):
            if hasattr(graph, 'metadata') and len(graph.metadata) > 0:
                graph_filename = graph.metadata[0].get('filename', '')
                if graph_filename in target_files:
                    indices.append(i)
        
        return indices
    
    # VulnerabilityDatasetã‚¯ãƒ©ã‚¹ã«è¿½åŠ 
    def get_initial_train_test_split(self, test_size=0.2):
        """æœ€åˆã®å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ï¼‰"""
        files = list(self.file_groups.keys())
        
        if len(files) < 2:
            logger.warning("ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå°‘ãªã„ãŸã‚ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã«ä½¿ç”¨")
            return list(range(len(self.graphs))), []
        
        train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)
        
        train_indices = self._get_graph_indices_by_files(train_files)
        test_indices = self._get_graph_indices_by_files(test_files)
        
        logger.info(f"ğŸ“Š åˆæœŸåˆ†å‰²çµæœ:")
        logger.info(f"   å­¦ç¿’ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {len(train_files)} ({len(train_indices)} graphs)")
        logger.info(f"   ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {len(test_files)} ({len(test_indices)} graphs)")
        
        return train_indices, test_indices

    def get_cross_validation_splits_from_indices(self, train_indices, k_folds=5):
        """æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ã®äº¤å·®æ¤œè¨¼åˆ†å‰²"""
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        train_files = []
        for i in train_indices:
            if hasattr(self.graphs[i], 'metadata') and len(self.graphs[i].metadata) > 0:
                filename = self.graphs[i].metadata[0].get('filename', '')
                if filename and filename not in train_files:
                    train_files.append(filename)
        
        if len(train_files) < k_folds:
            logger.warning(f"å­¦ç¿’ç”¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°({len(train_files)})ãŒfoldæ•°({k_folds})ã‚ˆã‚Šå°‘ãªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«èª¿æ•´")
            k_folds = len(train_files)
        
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        
        splits = []
        for fold, (train_files_idx, val_files_idx) in enumerate(kf.split(train_files)):
            fold_train_files = [train_files[i] for i in train_files_idx]
            fold_val_files = [train_files[i] for i in val_files_idx]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿å†…ã§ã®åˆ†å‰²ï¼‰
            fold_train_indices = []
            fold_val_indices = []
            
            for i in train_indices:
                if hasattr(self.graphs[i], 'metadata') and len(self.graphs[i].metadata) > 0:
                    filename = self.graphs[i].metadata[0].get('filename', '')
                    if filename in fold_train_files:
                        fold_train_indices.append(i)
                    elif filename in fold_val_files:
                        fold_val_indices.append(i)
            
            splits.append({
                'fold': fold + 1,
                'train_indices': fold_train_indices,
                'val_indices': fold_val_indices,
                'train_files': fold_train_files,
                'val_files': fold_val_files
            })
            
            logger.info(f"Fold {fold + 1}: Train={len(fold_train_indices)}, Val={len(fold_val_indices)}")
        
        return splits
    


class MultiTaskVulnerabilityGNN(nn.Module):
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯è„†å¼±æ€§æ¤œå‡ºGNNï¼ˆCWEåˆ†é¡ + ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡ºï¼‰"""
    
    def __init__(self, input_dim, hidden_dim=128, num_layers=3, num_classes=5, dropout=0.1):
        super(MultiTaskVulnerabilityGNN, self).__init__()
        
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        
        # å…±æœ‰GNNå±¤ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        
        # å…¥åŠ›å±¤
        self.convs.append(GATConv(input_dim, hidden_dim, heads=4, concat=True))
        self.batch_norms.append(BatchNorm(hidden_dim * 4))
        
        # éš ã‚Œå±¤
        for _ in range(num_layers - 2):
            self.convs.append(GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True))
            self.batch_norms.append(BatchNorm(hidden_dim * 4))
        
        # å‡ºåŠ›å±¤ï¼ˆç‰¹å¾´æŠ½å‡ºå®Œäº†ï¼‰
        self.convs.append(GATConv(hidden_dim * 4, hidden_dim * 4, heads=1, concat=False))
        self.batch_norms.append(BatchNorm(hidden_dim * 4))
        
        final_hidden_dim = hidden_dim * 4
        
        # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®åˆ†é¡å™¨
        # 1. CWEåˆ†é¡å™¨
        self.cwe_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(final_hidden_dim, final_hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(final_hidden_dim // 2, 1)
            ) for _ in range(num_classes)
        ])
        
        # 2. is_trigger_lineåˆ†é¡å™¨
        self.trigger_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(final_hidden_dim, final_hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(final_hidden_dim // 2, 1)
            ) for _ in range(num_classes)
        ])
        
        self.dropout = dropout
    
    def forward(self, x, edge_index, edge_attr=None, batch=None):
        # å…±æœ‰ç‰¹å¾´æŠ½å‡º
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index, edge_attr)
            x = self.batch_norms[i](x)
            if i < len(self.convs) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        # ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®äºˆæ¸¬
        # 1. CWEåˆ†é¡
        cwe_outputs = []
        for classifier in self.cwe_classifiers:
            cwe_outputs.append(classifier(x))
        cwe_predictions = torch.cat(cwe_outputs, dim=1)
        
        # 2. is_trigger_lineäºˆæ¸¬
        trigger_outputs = []
        for classifier in self.trigger_classifiers:
            trigger_outputs.append(classifier(x))
        trigger_predictions = torch.cat(trigger_outputs, dim=1)
        
        return cwe_predictions, trigger_predictions


class VulnerabilityTrainer:
    """è„†å¼±æ€§æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆäº¤å·®æ¤œè¨¼å¯¾å¿œï¼‰"""
    
    def __init__(self, approach='multi', device='cpu'):
        self.approach = approach
        self.device = device
        self.all_target_cwes = ['CWE-416', 'CWE-119', 'CWE-476', 'CWE-190', 'CWE-78']
        
        if approach == 'multi':
            self.target_cwes = self.all_target_cwes
            self.num_classes = 5
        elif approach in self.all_target_cwes:
            self.target_cwes = [approach]
            self.num_classes = 1
        else:
            raise ValueError(f"ç„¡åŠ¹ãªapproach: {approach}")
        
        self.model = None
        self.training_history = []
        
        logger.info(f"ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–: approach={approach}, device={device}")
    
    def setup_model(self, input_dim=512, **model_kwargs):
        """ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.model = MultiTaskVulnerabilityGNN(
            input_dim=input_dim,
            num_classes=self.num_classes,
            **model_kwargs
        ).to(self.device)
        
        logger.info(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«æ§‹æˆ: {self.num_classes}ã‚¯ãƒ©ã‚¹, ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        return self.model
    
    def cross_validate(self, dataset, k_folds=5, epochs=50, batch_size=16, learning_rate=0.001):
        """äº¤å·®æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info(f"ğŸ”„ äº¤å·®æ¤œè¨¼é–‹å§‹: {k_folds}-fold")
        
        cv_splits = dataset.get_cross_validation_splits(k_folds)
        fold_results = []
        
        for split in cv_splits:
            logger.info(f"ğŸ“Š Fold {split['fold']} / {k_folds}")
            
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æº–å‚™
            train_graphs = [dataset.graphs[i] for i in split['train_indices']]
            val_graphs = [dataset.graphs[i] for i in split['val_indices']]
            
            if len(train_graphs) == 0 or len(val_graphs) == 0:
                logger.warning(f"Fold {split['fold']}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.setup_model()
            
            # å­¦ç¿’å®Ÿè¡Œ
            fold_history = self._train_single_fold(
                train_graphs, val_graphs, epochs, batch_size, learning_rate
            )
            
            # çµæœä¿å­˜
            fold_result = {
                'fold': split['fold'],
                'train_files': split['train_files'],
                'val_files': split['val_files'],
                'final_val_acc': fold_history['val_accuracies'][-1] if fold_history['val_accuracies'] else 0.0,
                'final_val_loss': fold_history['val_losses'][-1] if fold_history['val_losses'] else 0.0,
                'history': fold_history
            }
            fold_results.append(fold_result)
        
        # äº¤å·®æ¤œè¨¼çµæœã®çµ±è¨ˆ
        cv_summary = self._summarize_cross_validation(fold_results)
        
        return cv_summary, fold_results

    def _train_single_fold(self, train_graphs, val_graphs, epochs, batch_size, learning_rate):
        """å˜ä¸€foldå­¦ç¿’ï¼ˆç°¡æ½”ç‰ˆï¼‰"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)
        
        # æå¤±é–¢æ•°ãƒ»æœ€é©åŒ–å™¨
        cwe_criterion = nn.BCEWithLogitsLoss()
        trigger_criterion = nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        
        # å­¦ç¿’å±¥æ­´
        train_losses = []
        val_losses = []
        val_accuracies = []
        detailed_metrics_history = []
        
        for epoch in range(epochs):
            # è¨“ç·´
            train_loss = self._train_epoch(train_loader, optimizer, cwe_criterion, trigger_criterion)
            
            # æ¤œè¨¼ï¼ˆè©³ç´°è©•ä¾¡ï¼‰
            val_loss, val_acc, predictions = self._evaluate(
                val_loader, cwe_criterion, trigger_criterion, return_predictions=True
            )
            
            # å±¥æ­´ä¿å­˜
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)
            
            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆæœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰
            if epoch == epochs - 1:
                cwe_metrics = self.calculate_detailed_metrics(
                    predictions['cwe_labels'], predictions['cwe_preds'], 
                    task_name="CWE", class_names=self.target_cwes
                )
                trigger_metrics = self.calculate_detailed_metrics(
                    predictions['trigger_labels'], predictions['trigger_preds'], 
                    task_name="Trigger", class_names=self.target_cwes
                )
                
                epoch_metrics = {
                    'epoch': epoch + 1,
                    'cwe_metrics': cwe_metrics,
                    'trigger_metrics': trigger_metrics
                }
                detailed_metrics_history.append(epoch_metrics)
            
            # ãƒ­ã‚°å‡ºåŠ›ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
            if (epoch + 1) % 10 == 0:
                logger.info(f"  Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies,
            'detailed_metrics': detailed_metrics_history
        }


    def _log_detailed_metrics(self, metrics_data, prefix=""):
        """è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"    ğŸ“Š {prefix} è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        
        # CWEåˆ†é¡çµæœ
        cwe_found = False
        for cwe in self.target_cwes:
            cwe_key = f'CWE_{cwe}'
            if cwe_key in metrics_data['cwe_metrics']:
                metrics = metrics_data['cwe_metrics'][cwe_key]
                cm = metrics['confusion_matrix']
                logger.info(f"      {cwe}: Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                logger.info(f"        {cwe} æ··åŒè¡Œåˆ—:")
                logger.info(f"          TN={cm['TN']}, FP={cm['FP']}")
                logger.info(f"          FN={cm['FN']}, TP={cm['TP']}")
                cwe_found = True
        
        if not cwe_found:
            logger.info(f"      CWEåˆ†é¡: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡ºçµæœ
        trigger_found = False
        for cwe in self.target_cwes:
            trigger_key = f'Trigger_{cwe}'
            if trigger_key in metrics_data['trigger_metrics']:
                metrics = metrics_data['trigger_metrics'][trigger_key]
                cm = metrics['confusion_matrix']
                logger.info(f"      {cwe} (Trigger): Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                logger.info(f"        {cwe} ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ··åŒè¡Œåˆ—:")
                logger.info(f"          TN={cm['TN']}, FP={cm['FP']}")
                logger.info(f"          FN={cm['FN']}, TP={cm['TP']}")
                trigger_found = True
        
        if not trigger_found:
            logger.info(f"      ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡º: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¿½åŠ 
        logger.debug(f"      CWE metrics keys: {list(metrics_data['cwe_metrics'].keys())}")
        logger.debug(f"      Trigger metrics keys: {list(metrics_data['trigger_metrics'].keys())}")



    
    def _train_epoch(self, train_loader, optimizer, cwe_criterion, trigger_criterion):
        """1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’"""
        self.model.train()
        total_loss = 0
        
        for batch in train_loader:
            batch = batch.to(self.device)
            optimizer.zero_grad()
            
            # äºˆæ¸¬
            cwe_pred, trigger_pred = self.model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
            
            # ãƒ©ãƒ™ãƒ«åˆ†é›¢
            batch_labels = batch.y
            cwe_labels = batch_labels[:, :self.num_classes]
            trigger_labels = batch_labels[:, self.num_classes:self.num_classes*2]
            
            # æå¤±è¨ˆç®—
            cwe_loss = cwe_criterion(cwe_pred, cwe_labels)
            trigger_loss = trigger_criterion(trigger_pred, trigger_labels)
            total_loss_batch = cwe_loss + trigger_loss
            
            # é€†ä¼æ’­
            total_loss_batch.backward()
            optimizer.step()
            
            total_loss += total_loss_batch.item()
        
        return total_loss / len(train_loader)
    
    def _evaluate(self, loader, cwe_criterion, trigger_criterion, return_predictions=False):
        """è©•ä¾¡ï¼ˆæ··åŒè¡Œåˆ—å¯¾å¿œç‰ˆï¼‰"""
        self.model.eval()
        total_loss = 0
        
        # äºˆæ¸¬çµæœã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        all_cwe_preds = []
        all_cwe_labels = []
        all_trigger_preds = []
        all_trigger_labels = []
        
        with torch.no_grad():
            for batch in loader:
                batch = batch.to(self.device)
                
                # äºˆæ¸¬
                cwe_pred, trigger_pred = self.model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
                
                # ãƒ©ãƒ™ãƒ«åˆ†é›¢
                batch_labels = batch.y
                cwe_labels = batch_labels[:, :self.num_classes]
                trigger_labels = batch_labels[:, self.num_classes:self.num_classes*2]
                
                # æå¤±
                cwe_loss = cwe_criterion(cwe_pred, cwe_labels)
                trigger_loss = trigger_criterion(trigger_pred, trigger_labels)
                total_loss += (cwe_loss + trigger_loss).item()
                
                # äºˆæ¸¬å€¤ã‚’0/1ã«å¤‰æ›
                cwe_preds = (torch.sigmoid(cwe_pred) > 0.5).int()
                trigger_preds = (torch.sigmoid(trigger_pred) > 0.5).int()
                
                # ãƒªã‚¹ãƒˆã«è¿½åŠ 
                all_cwe_preds.extend(cwe_preds.cpu().numpy())
                all_cwe_labels.extend(cwe_labels.int().cpu().numpy())
                all_trigger_preds.extend(trigger_preds.cpu().numpy())
                all_trigger_labels.extend(trigger_labels.int().cpu().numpy())
        
        # numpyé…åˆ—ã«å¤‰æ›
        all_cwe_preds = np.array(all_cwe_preds)
        all_cwe_labels = np.array(all_cwe_labels)
        all_trigger_preds = np.array(all_trigger_preds)
        all_trigger_labels = np.array(all_trigger_labels)
        
        # ç²¾åº¦è¨ˆç®—
        cwe_accuracy = accuracy_score(all_cwe_labels.flatten(), all_cwe_preds.flatten())
        trigger_accuracy = accuracy_score(all_trigger_labels.flatten(), all_trigger_preds.flatten())
        overall_accuracy = (cwe_accuracy + trigger_accuracy) / 2
        
        avg_loss = total_loss / len(loader)
        
        if return_predictions:
            return avg_loss, overall_accuracy, {
                'cwe_preds': all_cwe_preds,
                'cwe_labels': all_cwe_labels,
                'trigger_preds': all_trigger_preds,
                'trigger_labels': all_trigger_labels,
                'cwe_accuracy': cwe_accuracy,
                'trigger_accuracy': trigger_accuracy
            }
        
        return avg_loss, overall_accuracy

    def calculate_detailed_metrics(self, y_true, y_pred, task_name="", class_names=None):
        """è©³ç´°ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆæ··åŒè¡Œåˆ—å«ã‚€ï¼‰"""
        if class_names is None:
            class_names = self.target_cwes
        
        logger.debug(f"calculate_detailed_metrics: task={task_name}, classes={class_names}")
        logger.debug(f"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}")
        
        results = {}
        
        # ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ã®å ´åˆ
        if y_true.shape[1] > 1:
            # å„ã‚¯ãƒ©ã‚¹ã”ã¨ã®æ··åŒè¡Œåˆ—
            cm_per_class = multilabel_confusion_matrix(y_true, y_pred)
            logger.debug(f"multilabel_confusion_matrix shape: {cm_per_class.shape}")
            
            for i, class_name in enumerate(class_names):
                if i < len(cm_per_class):
                    tn, fp, fn, tp = cm_per_class[i].ravel()
                    logger.debug(f"Class {class_name}: TN={tn}, FP={fp}, FN={fn}, TP={tp}")
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
                    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
                    
                    results[f'{task_name}_{class_name}'] = {
                        'confusion_matrix': {
                            'TN': int(tn), 'FP': int(fp), 
                            'FN': int(fn), 'TP': int(tp)
                        },
                        'precision': precision,
                        'recall': recall,
                        'f1_score': f1,
                        'specificity': specificity,
                        'accuracy': accuracy
                    }
            
            # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
            overall_accuracy = accuracy_score(y_true, y_pred)
            hamming = hamming_loss(y_true, y_pred)
            
            results[f'{task_name}_overall'] = {
                'accuracy': overall_accuracy,
                'hamming_loss': hamming
            }
        
        else:
            # ãƒã‚¤ãƒŠãƒªåˆ†é¡ã®å ´åˆ
            y_true_flat = y_true.flatten()
            y_pred_flat = y_pred.flatten()
            
            logger.debug(f"Binary classification: y_true unique={np.unique(y_true_flat)}, y_pred unique={np.unique(y_pred_flat)}")
            
            # ãƒ‡ãƒ¼ã‚¿ã«å°‘ãªãã¨ã‚‚1ã¤ã®æ­£ä¾‹ãƒ»è² ä¾‹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            unique_true = np.unique(y_true_flat)
            unique_pred = np.unique(y_pred_flat)
            
            if len(unique_true) == 1 and len(unique_pred) == 1:
                # å®Œå…¨ã«å˜ä¸€ã‚¯ãƒ©ã‚¹ã®å ´åˆ
                if unique_true[0] == 0:  # å…¨ã¦è² ä¾‹
                    tn, fp, fn, tp = len(y_true_flat), 0, 0, 0
                else:  # å…¨ã¦æ­£ä¾‹
                    tn, fp, fn, tp = 0, 0, 0, len(y_true_flat)
            else:
                # é€šå¸¸ã®æ··åŒè¡Œåˆ—è¨ˆç®—
                cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1])
                logger.debug(f"Confusion matrix: {cm}")
                if cm.shape == (2, 2):
                    tn, fp, fn, tp = cm.ravel()
                elif cm.shape == (1, 1):
                    # å˜ä¸€ã‚¯ãƒ©ã‚¹ã®ã¿å­˜åœ¨ã™ã‚‹å ´åˆ
                    if len(unique_true) == 1 and unique_true[0] == 0:
                        tn, fp, fn, tp = cm[0, 0], 0, 0, 0
                    else:
                        tn, fp, fn, tp = 0, 0, 0, cm[0, 0]
                else:
                    tn, fp, fn, tp = 0, 0, 0, 0
            
            logger.debug(f"Final binary metrics: TN={tn}, FP={fp}, FN={fn}, TP={tp}")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
            
            # ãƒã‚¤ãƒŠãƒªåˆ†é¡ã§ã¯ class_names[0] ã‚’ä½¿ç”¨
            key_name = f'{task_name}_{class_names[0]}' if class_names else task_name
            results[key_name] = {
                'confusion_matrix': {
                    'TN': int(tn), 'FP': int(fp), 
                    'FN': int(fn), 'TP': int(tp)
                },
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'specificity': specificity,
                'accuracy': accuracy
            }
        
        logger.debug(f"Results keys: {list(results.keys())}")
        return results


    
    def _summarize_cross_validation(self, fold_results):
        """äº¤å·®æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼"""
        if not fold_results:
            return {"error": "äº¤å·®æ¤œè¨¼çµæœãŒã‚ã‚Šã¾ã›ã‚“"}
        
        accuracies = [result['final_val_acc'] for result in fold_results]
        losses = [result['final_val_loss'] for result in fold_results]
        
        summary = {
            'num_folds': len(fold_results),
            'mean_accuracy': np.mean(accuracies),
            'std_accuracy': np.std(accuracies),
            'mean_loss': np.mean(losses),
            'std_loss': np.std(losses),
            'accuracies': accuracies,
            'losses': losses,
            'best_fold': max(fold_results, key=lambda x: x['final_val_acc'])['fold'],
            'worst_fold': min(fold_results, key=lambda x: x['final_val_acc'])['fold']
        }
        
        logger.info(f"ğŸ“Š äº¤å·®æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼:")
        logger.info(f"   å¹³å‡ç²¾åº¦: {summary['mean_accuracy']:.4f} Â± {summary['std_accuracy']:.4f}")
        logger.info(f"   å¹³å‡æå¤±: {summary['mean_loss']:.4f} Â± {summary['std_loss']:.4f}")
        logger.info(f"   æœ€è‰¯Fold: {summary['best_fold']}, æœ€æ‚ªFold: {summary['worst_fold']}")
        
        return summary
    
    def train_model(self, train_graphs, val_graphs, epochs=100, batch_size=32, learning_rate=0.001):
        """é€šå¸¸å­¦ç¿’ï¼ˆæœ€çµ‚è©•ä¾¡ä»˜ãï¼‰"""
        if self.model is None:
            self.setup_model()
        
        logger.info(f"ğŸš€ å­¦ç¿’é–‹å§‹: {epochs}ã‚¨ãƒãƒƒã‚¯")
        
        history = self._train_single_fold(train_graphs, val_graphs, epochs, batch_size, learning_rate)
        self.training_history = history
        
        # æœ€çµ‚è©•ä¾¡ã®å®Ÿè¡Œ
        final_evaluation = None
        if val_graphs:
            logger.info("ğŸ“Š æœ€çµ‚è©•ä¾¡å®Ÿè¡Œä¸­...")
            
            val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)
            cwe_criterion = nn.BCEWithLogitsLoss()
            trigger_criterion = nn.BCEWithLogitsLoss()
            
            # è©³ç´°è©•ä¾¡ã®å®Ÿè¡Œ
            final_loss, final_acc, predictions = self._evaluate(
                val_loader, cwe_criterion, trigger_criterion, return_predictions=True
            )
            
            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            cwe_metrics = self.calculate_detailed_metrics(
                predictions['cwe_labels'], predictions['cwe_preds'], 
                task_name="CWE", class_names=self.target_cwes
            )
            trigger_metrics = self.calculate_detailed_metrics(
                predictions['trigger_labels'], predictions['trigger_preds'], 
                task_name="Trigger", class_names=self.target_cwes
            )
            
            final_evaluation = {
                'final_loss': final_loss,
                'final_accuracy': final_acc,
                'cwe_metrics': cwe_metrics,
                'trigger_metrics': trigger_metrics,
                'predictions': predictions
            }
            
            # çµæœè¡¨ç¤º
            self._log_final_evaluation(final_evaluation)
        else:
            logger.info("âš ï¸ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€æœ€çµ‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        logger.info("âœ… å­¦ç¿’å®Œäº†!")
        return history, final_evaluation

    def _log_final_evaluation(self, evaluation):
        """æœ€çµ‚è©•ä¾¡çµæœã®ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"\nğŸ¯ æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ:")
        logger.info(f"  ç·åˆç²¾åº¦: {evaluation['final_accuracy']:.4f}")
        logger.info(f"  ç·åˆæå¤±: {evaluation['final_loss']:.4f}")
        
        # CWEåˆ†é¡çµæœã®è©³ç´°è¡¨ç¤º
        logger.info(f"\nğŸ“ˆ CWEåˆ†é¡ æœ€çµ‚çµæœ:")
        for cwe in self.target_cwes:
            cwe_key = f'CWE_{cwe}'
            if cwe_key in evaluation['cwe_metrics']:
                metrics = evaluation['cwe_metrics'][cwe_key]
                cm = metrics['confusion_matrix']
                logger.info(f"  {cwe}: Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                logger.info(f"")
                logger.info(f"                    {cwe} æ··åŒè¡Œåˆ—:")
                logger.info(f"    TN={cm['TN']}, FP={cm['FP']}")
                logger.info(f"    FN={cm['FN']}, TP={cm['TP']}")
                logger.info("")
        
        # ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡ºçµæœã®è©³ç´°è¡¨ç¤º
        logger.info(f"ğŸ¯ ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡º æœ€çµ‚çµæœ:")
        for cwe in self.target_cwes:
            trigger_key = f'Trigger_{cwe}'
            if trigger_key in evaluation['trigger_metrics']:
                metrics = evaluation['trigger_metrics'][trigger_key]
                cm = metrics['confusion_matrix']
                logger.info(f"  {cwe} (Trigger): Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                logger.info(f"")
                logger.info(f"                    {cwe} ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ··åŒè¡Œåˆ—:")
                logger.info(f"    TN={cm['TN']}, FP={cm['FP']}")
                logger.info(f"    FN={cm['FN']}, TP={cm['TP']}")
                logger.info("")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®è¡¨ç¤º
        predictions = evaluation['predictions']
        cwe_labels_sum = np.sum(predictions['cwe_labels'], axis=0)
        trigger_labels_sum = np.sum(predictions['trigger_labels'], axis=0)
        cwe_preds_sum = np.sum(predictions['cwe_preds'], axis=0)
        trigger_preds_sum = np.sum(predictions['trigger_preds'], axis=0)
        
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
        for i, cwe in enumerate(self.target_cwes):
            if i < len(cwe_labels_sum):
                logger.info(f"  {cwe}:")
                logger.info(f"    å®Ÿéš›ã®è„†å¼±æ€§: {int(cwe_labels_sum[i])}ä»¶")
                logger.info(f"    äºˆæ¸¬ã—ãŸè„†å¼±æ€§: {int(cwe_preds_sum[i])}ä»¶")
                logger.info(f"    å®Ÿéš›ã®ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³: {int(trigger_labels_sum[i])}ä»¶")
                logger.info(f"    äºˆæ¸¬ã—ãŸãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³: {int(trigger_preds_sum[i])}ä»¶")

    
    def predict(self, test_graphs, confidence_threshold=0.5):
        """äºˆæ¸¬å®Ÿè¡Œ"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.model.eval()
        test_loader = DataLoader(test_graphs, batch_size=1, shuffle=False)
        
        results = []
        
        logger.info(f"ğŸ¯ äºˆæ¸¬é–‹å§‹ ({len(test_graphs)}ã‚°ãƒ©ãƒ•)")
        
        with torch.no_grad():
            for i, batch in enumerate(test_loader):
                batch = batch.to(self.device)
                
                # äºˆæ¸¬
                cwe_pred, trigger_pred = self.model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)
                
                # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é©ç”¨
                cwe_probs = torch.sigmoid(cwe_pred).cpu().numpy()
                trigger_probs = torch.sigmoid(trigger_pred).cpu().numpy()
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                metadata = test_graphs[i].metadata if hasattr(test_graphs[i], 'metadata') else [{}]
                
                # å„ãƒãƒ¼ãƒ‰ï¼ˆè¡Œï¼‰ã§ã®äºˆæ¸¬çµæœ
                for node_idx, (cwe_prob, trigger_prob) in enumerate(zip(cwe_probs, trigger_probs)):
                    node_meta = metadata[node_idx] if node_idx < len(metadata) else {}
                    
                    # å„CWEã”ã¨ã®çµæœ
                    for cwe_idx, cwe_id in enumerate(self.target_cwes):
                        cwe_confidence = float(cwe_prob[cwe_idx])
                        trigger_confidence = float(trigger_prob[cwe_idx])
                        
                        # ä¿¡é ¼åº¦ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿è¨˜éŒ²
                        if cwe_confidence > confidence_threshold or trigger_confidence > confidence_threshold:
                            result = {
                                'filename': node_meta.get('filename', f'graph_{i}'),
                                'ir_line': node_meta.get('ir_line', node_idx),
                                'cwe_id': cwe_id,
                                'cwe_confidence': cwe_confidence,
                                'is_trigger_line': trigger_confidence > confidence_threshold,
                                'trigger_confidence': trigger_confidence,
                                'combined_score': (cwe_confidence + trigger_confidence) / 2
                            }
                            results.append(result)
        
        # çµæœã‚’ä¿¡é ¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x['combined_score'], reverse=True)
        
        logger.info(f"âœ… äºˆæ¸¬å®Œäº†: {len(results)}ä»¶ã®è„†å¼±æ€§æ¤œå‡º")
        
        return results
    
    def save_model(self, output_dir, model_name="vulnerability_model"):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if self.model is None:
            logger.warning("ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        os.makedirs(output_dir, exist_ok=True)
        
        model_path = os.path.join(output_dir, f"{model_name}_{self.approach}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'approach': self.approach,
            'num_classes': self.num_classes,
            'target_cwes': self.target_cwes,
            'training_history': self.training_history
        }, model_path)
        
        logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
        return model_path


def convert_to_json_serializable(obj):
    """JSON serializable ãªå‹ã«å¤‰æ›ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'cpu') and hasattr(obj, 'numpy'):  # torch.Tensor
        return obj.cpu().numpy().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    elif isinstance(obj, dict):
        return {key: convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_to_json_serializable(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        return convert_to_json_serializable(obj.__dict__)
    return obj



def save_results(results, output_dir, filename_prefix="results"):
    """çµæœä¿å­˜ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    os.makedirs(output_dir, exist_ok=True)
    
    # JSONå¤‰æ›å‡¦ç†ï¼ˆå†å¸°çš„ã«å¤‰æ›ï¼‰
    serializable_results = convert_to_json_serializable(results)
    
    # JSONä¿å­˜
    json_file = os.path.join(output_dir, f"{filename_prefix}.json")
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        # CSVä¿å­˜ï¼ˆå¯èƒ½ãªå ´åˆã®ã¿ï¼‰
        if serializable_results and isinstance(serializable_results, list) and len(serializable_results) > 0:
            # ãƒ•ãƒ©ãƒƒãƒˆãªè¾æ›¸ã®å ´åˆã®ã¿CSVåŒ–
            first_item = serializable_results[0]
            if isinstance(first_item, dict) and all(not isinstance(v, (dict, list)) for v in first_item.values()):
                csv_file = os.path.join(output_dir, f"{filename_prefix}.csv")
                df = pd.DataFrame(serializable_results)
                df.to_csv(csv_file, index=False, encoding='utf-8')
                logger.info(f"ğŸ’¾ çµæœä¿å­˜: {json_file}, {csv_file}")
            else:
                logger.info(f"ğŸ’¾ çµæœä¿å­˜: {json_file} (CSVå¤‰æ›ä¸å¯)")
        else:
            logger.info(f"ğŸ’¾ çµæœä¿å­˜: {json_file}")
        
    except Exception as e:
        logger.error(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: äºˆæ¸¬çµæœéƒ¨åˆ†ã‚’é™¤å¤–ã—ã¦ä¿å­˜
        try:
            filtered_results = []
            for result in results:
                if isinstance(result, dict):
                    filtered_result = {k: v for k, v in result.items() if k != 'test_predictions'}
                    filtered_results.append(filtered_result)
                else:
                    filtered_results.append(result)
            
            serializable_filtered = convert_to_json_serializable(filtered_results)
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_filtered, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ çµæœä¿å­˜ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰: {json_file}")
        except Exception as e2:
            logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜ã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")
    
    return json_file



def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='LLVM-IRè„†å¼±æ€§æ¤œå‡ºGNNãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - æœ€çµ‚ç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # äº¤å·®æ¤œè¨¼
  python script.py --data-path dataset.json --approach CWE-119 --mode validate --output-dir ../result
  
  # å­¦ç¿’
  python script.py --data-path dataset.json --approach multi --mode train --epochs 100
  
  # æ¤œçŸ¥
  python script.py --data-path dataset.json --approach CWE-119 --mode test --confidence-threshold 0.7
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('--data-path', required=True,
                       help='JSONãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    # æ¤œå‡ºæ–¹å¼
    parser.add_argument('--approach',
                       choices=['multi', 'CWE-416', 'CWE-119', 'CWE-476', 'CWE-190', 'CWE-78'],
                       default='multi',
                       help='æ¤œçŸ¥æ–¹å¼: multi(å…¨CWE) ã¾ãŸã¯ å€‹åˆ¥CWE-ID')
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument('--mode',
                       choices=['validate', 'train', 'test'],
                       default='train',
                       help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: validate(äº¤å·®æ¤œè¨¼), train(å­¦ç¿’), test(æ¤œçŸ¥)')
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    parser.add_argument('--output-dir', default='./output',
                       help='çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--epochs', type=int, default=50,
                       help='å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                       help='å­¦ç¿’ç‡')
    parser.add_argument('--k-folds', type=int, default=5,
                       help='äº¤å·®æ¤œè¨¼ã®foldæ•°')
    
    # äºˆæ¸¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--confidence-threshold', type=float, default=0.5,
                       help='äºˆæ¸¬ä¿¡é ¼åº¦ã—ãã„å€¤')
    
    args = parser.parse_args()
    
    try:
        logger.info(f"ğŸš€ GNNè„†å¼±æ€§æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–‹å§‹")
        logger.info(f"ğŸ“‹ è¨­å®š: mode={args.mode}, approach={args.approach}")
        logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿: {args.data_path}")
        logger.info(f"ğŸ“ å‡ºåŠ›: {args.output_dir}")
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        if not os.path.exists(args.data_path):
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.data_path}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        dataset = VulnerabilityDataset(approach=args.approach)
        num_graphs = dataset.load_data(args.data_path)
        
        if num_graphs == 0:
            raise ValueError("æœ‰åŠ¹ãªã‚°ãƒ©ãƒ•ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
        trainer = VulnerabilityTrainer(approach=args.approach, device=device)
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥å®Ÿè¡Œ
        if args.mode == 'validate':
            # äº¤å·®æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰: å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰² â†’ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§äº¤å·®æ¤œè¨¼ â†’ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆè©•ä¾¡
            logger.info(f"ğŸ”„ äº¤å·®æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰: {args.approach}")
            
            # 1. æœ€åˆã®å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            train_indices, test_indices = dataset.get_initial_train_test_split(test_size=0.2)
            
            if len(train_indices) == 0:
                raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            if len(test_indices) == 0:
                logger.warning("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã§äº¤å·®æ¤œè¨¼ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
                # å¾“æ¥ã®äº¤å·®æ¤œè¨¼å®Ÿè¡Œ
                cv_summary, fold_results = trainer.cross_validate(
                    dataset, 
                    k_folds=args.k_folds,
                    epochs=args.epochs,
                    batch_size=args.batch_size,
                    learning_rate=args.learning_rate
                )
                save_results([cv_summary], args.output_dir, f"cv_summary_{args.approach}")
                save_results(fold_results, args.output_dir, f"cv_detailed_{args.approach}")
                logger.info("âœ… å‡¦ç†å®Œäº†!")
                return 0
            
            # 2. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§äº¤å·®æ¤œè¨¼å®Ÿè¡Œ
            logger.info(f"\nğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§äº¤å·®æ¤œè¨¼å®Ÿè¡Œ:")
            cv_splits = dataset.get_cross_validation_splits_from_indices(train_indices, args.k_folds)
            fold_results = []
            
            best_fold = None
            best_fold_acc = 0
            
            for split in cv_splits:
                logger.info(f"ğŸ“Š Fold {split['fold']} / {args.k_folds}")
                
                # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æº–å‚™
                fold_train_graphs = [dataset.graphs[i] for i in split['train_indices']]
                fold_val_graphs = [dataset.graphs[i] for i in split['val_indices']]
                
                if len(fold_train_graphs) == 0 or len(fold_val_graphs) == 0:
                    logger.warning(f"Fold {split['fold']}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—")
                    continue
                
                # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
                trainer.setup_model()
                
                # å­¦ç¿’å®Ÿè¡Œ
                fold_history = trainer._train_single_fold(
                    fold_train_graphs, fold_val_graphs, args.epochs, args.batch_size, args.learning_rate
                )
                
                # æœ€çµ‚æ¤œè¨¼ç²¾åº¦
                final_val_acc = fold_history['val_accuracies'][-1] if fold_history['val_accuracies'] else 0.0
                
                # æœ€è‰¯foldè¿½è·¡
                if final_val_acc > best_fold_acc:
                    best_fold_acc = final_val_acc
                    best_fold = {
                        'fold_num': split['fold'],
                        'model_state': trainer.model.state_dict().copy(),
                        'accuracy': final_val_acc,
                        'history': fold_history
                    }
                
                # çµæœä¿å­˜
                fold_result = {
                    'fold': split['fold'],
                    'train_files': split['train_files'],
                    'val_files': split['val_files'],
                    'final_val_acc': final_val_acc,
                    'final_val_loss': fold_history['val_losses'][-1] if fold_history['val_losses'] else 0.0,
                    'history': fold_history
                }
                fold_results.append(fold_result)
            
            # 3. äº¤å·®æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼
            cv_summary = trainer._summarize_cross_validation(fold_results)
            logger.info(f"ğŸ“Š äº¤å·®æ¤œè¨¼å®Œäº†: æœ€è‰¯Fold {best_fold['fold_num']} (ç²¾åº¦: {best_fold_acc:.4f})")
            
            # 4. æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡
            logger.info(f"\nğŸ¯ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«(Fold {best_fold['fold_num']})ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡:")
            
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
            trainer.model.load_state_dict(best_fold['model_state'])
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
            test_graphs = [dataset.graphs[i] for i in test_indices]
            test_loader = DataLoader(test_graphs, batch_size=args.batch_size, shuffle=False)
            
            # ãƒ†ã‚¹ãƒˆè©•ä¾¡å®Ÿè¡Œ
            cwe_criterion = nn.BCEWithLogitsLoss()
            trigger_criterion = nn.BCEWithLogitsLoss()
            
            test_loss, test_acc, test_predictions = trainer._evaluate(
                test_loader, cwe_criterion, trigger_criterion, return_predictions=True
            )
            
            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            test_cwe_metrics = trainer.calculate_detailed_metrics(
                test_predictions['cwe_labels'], test_predictions['cwe_preds'], 
                task_name="CWE", class_names=trainer.target_cwes
            )
            test_trigger_metrics = trainer.calculate_detailed_metrics(
                test_predictions['trigger_labels'], test_predictions['trigger_preds'], 
                task_name="Trigger", class_names=trainer.target_cwes
            )
            
            # ãƒ†ã‚¹ãƒˆçµæœè¡¨ç¤º
            final_test_results = {
                'test_loss': test_loss,
                'test_accuracy': test_acc,
                'cwe_metrics': test_cwe_metrics,
                'trigger_metrics': test_trigger_metrics,
                'best_fold': best_fold['fold_num'],
                'best_fold_val_acc': best_fold_acc,
                'train_test_split': {
                    'train_graphs': len(train_indices),
                    'test_graphs': len(test_indices)
                }
            }
            
            # çµæœè¡¨ç¤º
            logger.info(f"ğŸ† æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ:")
            logger.info(f"  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: äº¤å·®æ¤œè¨¼ Fold {final_test_results['best_fold']} ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«")
            logger.info(f"  æ¤œè¨¼æ™‚ç²¾åº¦: {final_test_results['best_fold_val_acc']:.4f}")
            logger.info(f"  ãƒ†ã‚¹ãƒˆç²¾åº¦: {final_test_results['test_accuracy']:.4f}")
            logger.info(f"  ãƒ†ã‚¹ãƒˆæå¤±: {final_test_results['test_loss']:.4f}")
            
            # CWEåˆ†é¡çµæœã®è©³ç´°è¡¨ç¤º
            logger.info(f"\nğŸ“ˆ CWEåˆ†é¡ ãƒ†ã‚¹ãƒˆçµæœ:")
            for cwe in trainer.target_cwes:
                cwe_key = f'CWE_{cwe}'
                if cwe_key in final_test_results['cwe_metrics']:
                    metrics = final_test_results['cwe_metrics'][cwe_key]
                    cm = metrics['confusion_matrix']
                    logger.info(f"  {cwe}: Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                    logger.info(f"")
                    logger.info(f"                    {cwe} æ··åŒè¡Œåˆ—:")
                    logger.info(f"    TN={cm['TN']}, FP={cm['FP']}")
                    logger.info(f"    FN={cm['FN']}, TP={cm['TP']}")
                    logger.info("")
            
            # ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡ºçµæœã®è©³ç´°è¡¨ç¤º
            logger.info(f"ğŸ¯ ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ¤œå‡º ãƒ†ã‚¹ãƒˆçµæœ:")
            for cwe in trainer.target_cwes:
                trigger_key = f'Trigger_{cwe}'
                if trigger_key in final_test_results['trigger_metrics']:
                    metrics = final_test_results['trigger_metrics'][trigger_key]
                    cm = metrics['confusion_matrix']
                    logger.info(f"  {cwe} (Trigger): Acc={metrics['accuracy']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
                    logger.info(f"")
                    logger.info(f"                    {cwe} ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ··åŒè¡Œåˆ—:")
                    logger.info(f"    TN={cm['TN']}, FP={cm['FP']}")
                    logger.info(f"    FN={cm['FN']}, TP={cm['TP']}")
                    logger.info("")
            
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_path = trainer.save_model(args.output_dir, f"best_model_{args.approach}")
            
            # çµæœä¿å­˜
            save_results([cv_summary], args.output_dir, f"cv_summary_{args.approach}")
            save_results(fold_results, args.output_dir, f"cv_detailed_{args.approach}")
            save_results([final_test_results], args.output_dir, f"final_test_results_{args.approach}")
            
            logger.info(f"ğŸ’¾ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
            logger.info(f"ğŸ† æœ€çµ‚ãƒ†ã‚¹ãƒˆç²¾åº¦: {final_test_results['test_accuracy']:.4f}")

        elif args.mode == 'train':
            # é€šå¸¸å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰² â†’ å­¦ç¿’ â†’ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼
            logger.info(f"ğŸš€ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: {args.approach}")
            
            # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
            train_indices, test_indices = dataset.get_initial_train_test_split(test_size=0.2)
            
            if len(train_indices) == 0:
                raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            train_graphs = [dataset.graphs[i] for i in train_indices]
            test_graphs = [dataset.graphs[i] for i in test_indices] if test_indices else []
            
            logger.info(f"ğŸš€ å­¦ç¿’å®Ÿè¡Œ: Train={len(train_graphs)}, Test={len(test_graphs)}")
            
            # å­¦ç¿’å®Ÿè¡Œï¼ˆæœ€çµ‚è©•ä¾¡ä»˜ãï¼‰
            history, final_evaluation = trainer.train_model(
                train_graphs, test_graphs,
                epochs=args.epochs,
                batch_size=args.batch_size,
                learning_rate=args.learning_rate
            )
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_path = trainer.save_model(args.output_dir, f"trained_model_{args.approach}")
            
            # å­¦ç¿’å±¥æ­´ä¿å­˜
            save_results([history], args.output_dir, f"training_history_{args.approach}")
            
            # æœ€çµ‚è©•ä¾¡çµæœä¿å­˜
            if final_evaluation:
                save_results([final_evaluation], args.output_dir, f"final_evaluation_{args.approach}")
                logger.info(f"ğŸ’¾ æœ€çµ‚è©•ä¾¡çµæœä¿å­˜å®Œäº†")
                
                # äºˆæ¸¬çµæœã®è©³ç´°ä¿å­˜
                if 'predictions' in final_evaluation:
                    predictions_detail = {
                        'cwe_predictions': final_evaluation['predictions']['cwe_preds'].tolist(),
                        'cwe_labels': final_evaluation['predictions']['cwe_labels'].tolist(),
                        'trigger_predictions': final_evaluation['predictions']['trigger_preds'].tolist(),
                        'trigger_labels': final_evaluation['predictions']['trigger_labels'].tolist(),
                        'metadata': {
                            'approach': args.approach,
                            'target_cwes': trainer.target_cwes,
                            'test_graphs_count': len(test_graphs),
                            'total_nodes': len(final_evaluation['predictions']['cwe_preds'])
                        }
                    }
                    save_results([predictions_detail], args.output_dir, f"predictions_detail_{args.approach}")
            
            # å­¦ç¿’æ›²ç·šã®ç°¡æ˜“è¡¨ç¤º
            if history and 'val_accuracies' in history:
                logger.info(f"\nğŸ“ˆ å­¦ç¿’æ›²ç·šã‚µãƒãƒªãƒ¼:")
                val_accs = history['val_accuracies']
                train_losses = history['train_losses']
                val_losses = history['val_losses']
                
                logger.info(f"  åˆæœŸæ¤œè¨¼ç²¾åº¦: {val_accs[0]:.4f}")
                logger.info(f"  æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_accs[-1]:.4f}")
                logger.info(f"  æœ€é«˜æ¤œè¨¼ç²¾åº¦: {max(val_accs):.4f} (Epoch {val_accs.index(max(val_accs))+1})")
                
                # éå­¦ç¿’ã®ç°¡æ˜“åˆ¤å®š
                if len(val_losses) > 10:
                    recent_val_loss_trend = np.mean(val_losses[-5:]) - np.mean(val_losses[-10:-5])
                    if recent_val_loss_trend > 0.1:
                        logger.info(f"  âš ï¸ éå­¦ç¿’ã®å¯èƒ½æ€§: æ¤œè¨¼æå¤±ãŒä¸Šæ˜‡å‚¾å‘")
                    elif recent_val_loss_trend < -0.1:
                        logger.info(f"  âœ… è‰¯å¥½ãªå­¦ç¿’: æ¤œè¨¼æå¤±ãŒä½ä¸‹å‚¾å‘")
                    else:
                        logger.info(f"  ğŸ“Š å®‰å®šã—ãŸå­¦ç¿’: æ¤œè¨¼æå¤±ãŒå®‰å®š")
            
            # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼
            if final_evaluation:
                logger.info(f"\nğŸ† ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼:")
                logger.info(f"  ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {args.approach}")
                logger.info(f"  å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
                logger.info(f"  æœ€çµ‚ç²¾åº¦: {final_evaluation['final_accuracy']:.4f}")
                logger.info(f"  ä¿å­˜ãƒ¢ãƒ‡ãƒ«: {model_path}")
                
                # å„CWEã®æœ€é«˜F1ã‚¹ã‚³ã‚¢è¡¨ç¤º
                best_cwe_f1 = 0
                best_trigger_f1 = 0
                
                for cwe in trainer.target_cwes:
                    cwe_key = f'CWE_{cwe}'
                    if cwe_key in final_evaluation['cwe_metrics']:
                        f1 = final_evaluation['cwe_metrics'][cwe_key]['f1_score']
                        best_cwe_f1 = max(best_cwe_f1, f1)
                    
                    trigger_key = f'Trigger_{cwe}'
                    if trigger_key in final_evaluation['trigger_metrics']:
                        f1 = final_evaluation['trigger_metrics'][trigger_key]['f1_score']
                        best_trigger_f1 = max(best_trigger_f1, f1)
                
                logger.info(f"  æœ€é«˜CWEåˆ†é¡F1: {best_cwe_f1:.4f}")
                logger.info(f"  æœ€é«˜ãƒˆãƒªã‚¬ãƒ¼æ¤œå‡ºF1: {best_trigger_f1:.4f}")

        elif args.mode == 'test':
            # æ¤œçŸ¥ãƒ¢ãƒ¼ãƒ‰: ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å®Ÿéš›ã®æ¤œçŸ¥ã‚’å®Ÿè¡Œ
            logger.info(f"ğŸ¯ æ¤œçŸ¥ãƒ¢ãƒ¼ãƒ‰: {args.approach}")
            
            # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            model_patterns = [
                f"best_model_{args.approach}.pth",      # validateãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜
                f"trained_model_{args.approach}.pth",   # trainãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜
                f"vulnerability_model_{args.approach}.pth"  # å¾“æ¥ã®å‘½å
            ]
            
            model_path = None
            for pattern in model_patterns:
                candidate_path = os.path.join(args.output_dir, pattern)
                if os.path.exists(candidate_path):
                    model_path = candidate_path
                    break
            
            if model_path is None:
                logger.error(f"ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
                for pattern in model_patterns:
                    logger.error(f"  - {os.path.join(args.output_dir, pattern)}")
                raise FileNotFoundError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            logger.info(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            checkpoint = torch.load(model_path, map_location='cpu')
            trainer.setup_model()
            trainer.model.load_state_dict(checkpoint['model_state_dict'])
            
            logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            logger.info(f"  å­¦ç¿’æ™‚ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {checkpoint.get('approach', 'unknown')}")
            logger.info(f"  å¯¾è±¡CWE: {checkpoint.get('target_cwes', 'unknown')}")
            
            # äºˆæ¸¬å®Ÿè¡Œ
            results = trainer.predict(dataset.graphs, confidence_threshold=args.confidence_threshold)
            
            # çµæœè¡¨ç¤º
            logger.info(f"ğŸ“Š æ¤œçŸ¥çµæœã‚µãƒãƒªãƒ¼:")
            logger.info(f"   ç·æ¤œå‡ºæ•°: {len(results)}")
            
            cwe_counts = defaultdict(int)
            trigger_count = 0
            
            for result in results:
                cwe_counts[result['cwe_id']] += 1
                if result['is_trigger_line']:
                    trigger_count += 1
            
            logger.info(f"   ãƒˆãƒªã‚¬ãƒ¼ãƒ©ã‚¤ãƒ³æ•°: {trigger_count}")
            for cwe_id, count in cwe_counts.items():
                logger.info(f"   {cwe_id}: {count}ä»¶")
            
            # é«˜ä¿¡é ¼åº¦çµæœè¡¨ç¤º
            logger.info(f"\nğŸ”¥ é«˜ä¿¡é ¼åº¦æ¤œçŸ¥çµæœ Top 10:")
            for i, result in enumerate(results[:10]):
                trigger_mark = "ğŸ¯" if result['is_trigger_line'] else "  "
                logger.info(f"   {i+1:2d}. {trigger_mark} {result['filename']}:{result['ir_line']} "
                          f"- {result['cwe_id']} (ä¿¡é ¼åº¦: {result['combined_score']:.3f})")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ
            file_stats = defaultdict(int)
            for result in results:
                file_stats[result['filename']] += 1
            
            logger.info(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥æ¤œçŸ¥çµ±è¨ˆ Top 5:")
            sorted_files = sorted(file_stats.items(), key=lambda x: x[1], reverse=True)
            for i, (filename, count) in enumerate(sorted_files[:5]):
                logger.info(f"   {i+1}. {filename}: {count}ä»¶")
            
            # çµæœä¿å­˜
            save_results(results, args.output_dir, f"detection_results_{args.approach}")
            
            # çµ±è¨ˆã‚µãƒãƒªãƒ¼ä¿å­˜
            detection_summary = {
                'total_detections': len(results),
                'trigger_lines': trigger_count,
                'cwe_counts': dict(cwe_counts),
                'file_stats': dict(file_stats),
                'confidence_threshold': args.confidence_threshold,
                'model_used': model_path,
                'approach': args.approach
            }
            save_results([detection_summary], args.output_dir, f"detection_summary_{args.approach}")
        
        logger.info(f"âœ… å‡¦ç†å®Œäº†! çµæœã¯ {args.output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
